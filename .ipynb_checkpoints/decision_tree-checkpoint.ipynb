{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed39ee45-3884-4c25-b7a9-7a37c8e8c78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8687f334-c5d8-474f-a63e-dcb20d0524b2",
   "metadata": {},
   "source": [
    "\n",
    "- $C$ is the number of unique classes in $y$.\n",
    "- $p_i$ is the proportion of samples in class $i$ out of all samples in $y$.\n",
    "## Gini\n",
    "The Gini impurity for a given set of labels $y$ is calculated using the following formula:\n",
    "\n",
    "\n",
    "$$\\text{Gini}(y) = 1 - \\sum_{i=1}^{C} p_i^2$$\n",
    "\n",
    "\n",
    "\n",
    "This formula computes the Gini impurity by summing the squared proportions of each class and subtracting the sum from 1. The result represents how impure or mixed the classes are within the dataset.\n",
    "\n",
    "\n",
    "## Entropy\n",
    "The entropy for a given set of labels $y$ is calculated using the following formula:\n",
    "\n",
    "$$\n",
    "\\text{Entropy}(y) = - \\sum_{i=1}^{C} p_i \\cdot \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "\n",
    "This formula computes the entropy by summing the product of each class proportion $p_i$ and its logarithm to the base 2, then taking the negative of the sum. Entropy measures the uncertainty or randomness in the distribution of classes within the dataset.\n",
    "\n",
    "## Misclassification Error\n",
    "The misclassification error for a given set of labels $y$ is calculated using the following formula:\n",
    "\n",
    "$$\n",
    "\\text{Misclassification Error}(y) = 1 - \\max_{i=1}^{C} p_i\n",
    "$$\n",
    "\n",
    "This formula computes the misclassification error by finding the proportion of the majority class (the class with the highest proportion) and subtracting it from 1. The result represents the error rate or the proportion of misclassified samples within the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbb7ae2c-c458-4560-91bd-4d96b47b0b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, criterion='gini', max_depth=None, min_samples_split=2):\n",
    "        # Initialize the decision tree with specified hyperparameters.\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "\n",
    "    def _calculate_gini(self, y):\n",
    "        # Calculate Gini impurity for a given set of labels.\n",
    "        classes = np.unique(y)\n",
    "        gini = 1\n",
    "        for cls in classes:\n",
    "            p_cls = np.sum(y == cls) / len(y)\n",
    "            gini -= p_cls ** 2\n",
    "        return gini\n",
    "\n",
    "    def _calculate_misclassification_error(self, y):\n",
    "        classes = np.unique(y)\n",
    "        error = 1 - np.max([np.sum(y == cls) / len(y) for cls in classes])\n",
    "        return error\n",
    "\n",
    "    def _calculate_entropy(self, y):\n",
    "        # Calculate entropy for a given set of labels.\n",
    "        classes = np.unique(y)\n",
    "        entropy = 0\n",
    "        for cls in classes:\n",
    "            p_cls = np.sum(y == cls) / len(y)\n",
    "            entropy -= p_cls * np.log2(p_cls)\n",
    "        return entropy\n",
    "\n",
    "    def _calculate_criterion(self, y):\n",
    "        # Calculate impurity/criterion based on the specified criterion.\n",
    "        if self.criterion == 'gini':\n",
    "            return self._calculate_gini(y)\n",
    "        elif self.criterion == 'entropy':\n",
    "            return self._calculate_entropy(y)\n",
    "        elif self.criterion == 'mis_class':\n",
    "            return self._calculate_misclassification_error(y)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid criterion. Use 'gini' or 'entropy'.\")\n",
    "\n",
    "    def _split(self, X, y, feature_index, threshold):\n",
    "        # Split dataset based on a feature and a threshold.\n",
    "        left_mask = X[:, feature_index] <= threshold\n",
    "        right_mask = ~left_mask\n",
    "        return X[left_mask], X[right_mask], y[left_mask], y[right_mask]\n",
    "\n",
    "    def _find_best_split(self, X, y):\n",
    "        # Find the best feature and threshold to split the dataset.\n",
    "        n_features = X.shape[1]\n",
    "        best_gini = float('inf')\n",
    "        best_feature_index = None\n",
    "        best_threshold = None\n",
    "\n",
    "        for feature_index in range(n_features):\n",
    "            \n",
    "            # Check if the feature is continuous or discrete\n",
    "            if np.all(np.mod(X[:, feature_index], 1) == 0):\n",
    "                thresholds = np.unique(X[:, feature_index])\n",
    "            else:\n",
    "                # If the feature values are not all integers (continuous feature),\n",
    "                # generate 10 equally spaced thresholds between the minimum and maximum feature values.\n",
    "                thresholds = np.linspace(X[:, feature_index].min(), X[:, feature_index].max(), 15)\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                _, _, y_left, y_right = self._split(X, y, feature_index, threshold)\n",
    "                if len(y_left) < self.min_samples_split or len(y_right) < self.min_samples_split:\n",
    "                    continue\n",
    "                    \n",
    "                gini_left = self._calculate_criterion(y_left)\n",
    "                gini_right = self._calculate_criterion(y_right)\n",
    "                gini = (len(y_left) * gini_left + len(y_right) * gini_right) / len(y)\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_feature_index = feature_index\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature_index, best_threshold\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        # Recursively build the decision tree.\n",
    "        if depth == self.max_depth or len(np.unique(y)) == 1:\n",
    "            return Counter(y).most_common(1)[0][0]\n",
    "\n",
    "        best_feature_index, best_threshold = self._find_best_split(X, y)\n",
    "        if best_feature_index is None:\n",
    "            return Counter(y).most_common(1)[0][0]\n",
    "\n",
    "        X_left, X_right, y_left, y_right = self._split(X, y, best_feature_index, best_threshold)\n",
    "\n",
    "        node = {}\n",
    "        node['feature_index'] = best_feature_index\n",
    "        node['threshold'] = best_threshold\n",
    "        node['size'] = len(y)\n",
    "        node['left'] = self._build_tree(X_left, y_left, depth + 1)\n",
    "        node['right'] = self._build_tree(X_right, y_right, depth + 1)\n",
    "\n",
    "        return node\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Fit the decision tree to the training data.\n",
    "        self.tree_ = self._build_tree(X, y, 0)\n",
    "\n",
    "    def _predict_instance(self, x, tree):\n",
    "        # Predict the label of a single instance.\n",
    "        if isinstance(tree, dict):\n",
    "            feature_index = tree['feature_index']\n",
    "            if x[feature_index] <= tree['threshold']:\n",
    "                return self._predict_instance(x, tree['left'])\n",
    "            else:\n",
    "                return self._predict_instance(x, tree['right'])\n",
    "        else:\n",
    "            return tree\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict labels for multiple instances.\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            predictions.append(self._predict_instance(x, self.tree_))\n",
    "        return np.array(predictions)\n",
    "\n",
    "\n",
    "    def print_tree(self):\n",
    "        # Print the decision tree in a readable format.\n",
    "        self._print_node(self.tree_, 0)\n",
    "\n",
    "    def _print_node(self, node, depth):\n",
    "        # Recursively print nodes of the decision tree.\n",
    "        if isinstance(node, dict):\n",
    "            print(\"  \" * depth, f\"Feature: {node['feature_index']}, Threshold: {node['threshold']} , Size = {node['size']}\")\n",
    "            self._print_node(node['left'], depth + 1)\n",
    "            self._print_node(node['right'], depth + 1)\n",
    "        else:\n",
    "            print(\"  \" * depth, f\"Leaf node: Class {node}\")\n",
    "\n",
    "\n",
    "    def plot_tree_text(self, node=None, depth=0):\n",
    "        # Plot the decision tree in text format.\n",
    "        if node is None:\n",
    "            node = self.tree_\n",
    "\n",
    "        if isinstance(node, dict):\n",
    "            print(f\"{' ' * depth * 2} [X{node['feature_index']} <= {node['threshold']} , Size = {node['size']}\")\n",
    "            self.plot_tree_text(node['left'], depth + 1)\n",
    "            self.plot_tree_text(node['right'], depth + 1)\n",
    "        else:\n",
    "            print(f\"{' ' * depth * 2} [{node}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eed23867-7d17-45d1-9072-1c09ad21ebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = []\n",
    "data_y = []\n",
    "with open('data/iris.data', 'r') as file:\n",
    "    reader = csv.reader(file, delimiter=',')\n",
    "    for row in reader:\n",
    "        if len(row) > 0:\n",
    "            data_x.append(row[:-1])\n",
    "            data_y.append(row[-1])\n",
    "class_mapping = {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}\n",
    "\n",
    "X = np.array(data_x, dtype=np.float32)\n",
    "y = np.array([class_mapping[cls] for cls in data_y])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b6c6e79-2845-4b05-8b44-bee83f68a7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    split_ratio = 0.8\n",
    "    split_index = int(split_ratio * len(data_x))\n",
    "    X_train = X[:split_index]\n",
    "    y_train = y[:split_index]\n",
    "    X_test = X[split_index:]\n",
    "    y_test = y[split_index:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "574a9351-d103-47e8-bc59-94ec6a0fe103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of gini = 0.7333333333333333\n",
      "accuracy of entropy = 0.7333333333333333\n",
      "accuracy of mis_class = 0.9\n"
     ]
    }
   ],
   "source": [
    "criterions = ['gini', 'entropy', 'mis_class']\n",
    "dtrees = [DecisionTree(criterion=c) for c in criterions]\n",
    "\n",
    "for c, d in zip(criterions, dtrees):\n",
    "    d.fit(X_train, y_train)\n",
    "    accuracy = np.mean(d.predict(X_test) == y_test)\n",
    "    print('accuracy of', c, '=', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd75314-5516-4b29-b3b4-f3952246166d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
